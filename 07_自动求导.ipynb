{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对y=2x^Tx 关于列向量 x求导 ",
   "id": "6d31144b602d0add"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T11:56:40.823435Z",
     "start_time": "2025-05-13T11:56:40.808546Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在计算y关于向量x之前，需要一个地方保存梯度",
   "id": "8d409109285a6bbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:56:41.257125Z",
     "start_time": "2025-05-13T11:56:41.251837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.requires_grad_(True)  # 设为True，表示，要将x的梯度保存起来\n",
    "# 等价于 x = torch.arange(4.0, require_grad=True)\n",
    "\n",
    "print(x.grad) # 访问梯度， 默认为None"
   ],
   "id": "d190fb7fc0903762",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "计算y，向量点乘：torch.dot()",
   "id": "98db7ff50829fc19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:56:50.297657Z",
     "start_time": "2025-05-13T11:56:50.277385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 2 * torch.dot(x, x)  # Pytorch隐式创建计算图，grad_fn=<MulBackward0>保存梯度函数，这个里面保存了y是x的函数的信息\n",
    "y"
   ],
   "id": "2f55dced8e5a193e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过调用反向传播函数(.backward())，自动计算y关于x每个分量的梯度",
   "id": "516f7dfd83b2a883"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:57:00.065491Z",
     "start_time": "2025-05-13T11:57:00.051930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y.backward()\n",
    "x.grad"
   ],
   "id": "5c778fa1f7dcf002",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:57:00.976219Z",
     "start_time": "2025-05-13T11:57:00.959471Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad == 4*x # 4*x就是y对x求导的结果 ",
   "id": "8a55f514e3bab1f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "默认情况，Pytorch会累积梯度，所以如果计算新的，需要清除之前的梯度",
   "id": "fbae92293bfc830a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:57:03.929044Z",
     "start_time": "2025-05-13T11:57:03.903790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad # 此时x的梯度是错误的，因为y=sum(x)对x求导，梯度应该是全1，现在的结果是因为保存了前面的 0 4 8 12"
   ],
   "id": "f473797863758064",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  5.,  9., 13.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:57:05.357440Z",
     "start_time": "2025-05-13T11:57:05.346362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.grad.zero_() # 清空现在的梯度\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad "
   ],
   "id": "d257fb70021c9126",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "y如果是向量： 例如y=x*x\n",
    "按理说向量y对向量x求导，是一个矩阵，但深度学习中我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和"
   ],
   "id": "e6907460d7a721fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:57:06.812395Z",
     "start_time": "2025-05-13T11:57:06.794418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对非标量调用`backward()`需要传入一个`gradient` 参数，用于指定微分函数\n",
    "x.grad.zero_()\n",
    "y = x*x\n",
    "# 机器学习，深度学习中很少会对向量去求导，一般都是对标量求backward(loss.backward()) \n",
    "# 所以这里做一个求和，变成标量\n",
    "y.sum().backward()  # 为什么sum()????? 因为sum()的导数是1么?这样又不影响结果，又能变成标量\n",
    "x.grad"
   ],
   "id": "52bc0fa5fe02f1f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "将某些计算移动到记录的计算图之外\n",
    "有啥用？？？？？"
   ],
   "id": "779948b139ab861a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:03:11.607109Z",
     "start_time": "2025-05-13T12:03:11.584204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach() # detach后，把y当一个常数，而不是关于x的函数\n",
    "z = u*x\n",
    "\n",
    "z.sum().backward() # z对x求导，把u当一个常数进行\n",
    "x.grad == u # 当在网络中需要把一些参数固定住时很有用"
   ],
   "id": "7d7c06588b3406a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:03:11.980442Z",
     "start_time": "2025-05-13T12:03:11.966433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "y, x, x.grad, x.grad ==2*x # y=x^2的导数是2x"
   ],
   "id": "3512422ac5db0c81",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 2., 4., 6.]),\n",
       " tensor([True, True, True, True]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "y如果不是一个简单的函数，而是一个python控制流(例如 条件。循环或任意函数调用)，也一样可以用backward求导，得到变量的梯度",
   "id": "f45048c2152a3c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:36:13.579149Z",
     "start_time": "2025-05-13T12:36:13.565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    # 控制流里每个分支都是b的函数，所以可以反向传播求梯度\n",
    "    # 这也是隐式构造计算图比显示构造的好处，控制流也可以直接求导，如果是显示的话，必须提前定义好变量，那这种分支可能就不方便了\n",
    "    # 但反过来讲，这样比显示的慢一些\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True) \n",
    "# d = f(torch.tensor(1000.))\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "# 为什么a的梯度(d对a求导)是d/a?\n",
    "# 因为：d是a的线性函数，d=ma, 所以d对a的导数为m，而m等于d/a，所以d对a的导数就是d/a\n",
    "\n",
    "a,d, d/a, a.grad == d/a, a.grad "
   ],
   "id": "825f8dac93b94f5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1055, requires_grad=True),\n",
       " tensor(-172922.4375, grad_fn=<MulBackward0>),\n",
       " tensor(1638400., grad_fn=<DivBackward0>),\n",
       " tensor(True),\n",
       " tensor(1638400.))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:16:42.328512Z",
     "start_time": "2025-05-13T12:16:42.315962Z"
    }
   },
   "cell_type": "code",
   "source": " # torch.randn(size=()).shape   # size=()生成一个随机标量",
   "id": "fa1d5a26b05c11dd",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:40:47.595509Z",
     "start_time": "2025-05-13T12:40:47.568911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(a):\n",
    "    b = a ** 2  # !!!!!如果改为b=a^2!!!!!!!!!!\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    # 控制流里每个分支都是b的函数，所以可以反向传播求梯度\n",
    "    # 这也是隐式构造计算图比显示构造的好处，控制流也可以直接求导，如果是显示的话，必须提前定义好变量，那这种分支可能就不方便了\n",
    "    # 但反过来讲，这样比显示的慢一些\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True) \n",
    "# d = f(torch.tensor(1000.))\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "# d=ma^2 d对a的导数为2ma，而2ma等于d/a*2，所以a的梯度为d/a*2\n",
    "a.grad==d/a*2"
   ],
   "id": "ed828a3dbedc8a45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "122fee465fe626b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
